{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Twk23\\AppData\\Local\\Temp\\ipykernel_21360\\3520892795.py:15: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  y_train = y_train.replace({\"Over\": True, \"Under\": False})\n",
      "C:\\Users\\Twk23\\AppData\\Local\\Temp\\ipykernel_21360\\3520892795.py:29: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  test_y = test_y.replace({\"Over\": 1, \"Under\": 0})\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "scalar = MinMaxScaler()\n",
    "\n",
    "X_train = pd.read_csv('trainX.csv')  \n",
    "y_train = pd.read_csv('trainY.csv')\n",
    "\n",
    "y_train = y_train.replace({\"Over\": True, \"Under\": False})\n",
    "\n",
    "\n",
    "X_train = X_train.to_numpy(dtype=np.float32)\n",
    "y_train = y_train.to_numpy(dtype=np.float32)\n",
    "\n",
    "X_train = scalar.fit_transform(X_train)\n",
    "# X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "# y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1) \n",
    "\n",
    "\n",
    "test_X = pd.read_csv('testX.csv')\n",
    "test_y = pd.read_csv('testY.csv')\n",
    "\n",
    "test_y = test_y.replace({\"Over\": 1, \"Under\": 0})\n",
    "\n",
    "test_X = test_X.to_numpy(dtype=np.float32)\n",
    "test_y = test_y.to_numpy(dtype=np.float32)\n",
    "\n",
    "test_X = scalar.transform(test_X)\n",
    "\n",
    "def toSequences(seq_size, X, y):\n",
    "    X_seq = []\n",
    "    y_seq = []\n",
    "    for i in range(len(X) - seq_size):\n",
    "        X_seq.append(X[i:i+seq_size])\n",
    "        y_seq.append(y[i+seq_size-1]) \n",
    "    return torch.tensor(X_seq, dtype=torch.float32).view(-1, seq_size, 19), torch.tensor(y_seq, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "X_train, y_train = toSequences(20, X_train, y_train)\n",
    "X_test, y_test = toSequences(20, test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2608, 20, 19]) torch.Size([2608, 1])\n",
      "torch.Size([855, 20, 19]) torch.Size([855, 1])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CNN_LSTM_Model(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers, cnn_out_channels, kernel_size, output_size):\n",
    "#         super(CNN_LSTM_Model, self).__init__()\n",
    "        \n",
    "#         # CNN layers\n",
    "#         self.cnn = nn.Conv1d(in_channels=input_size, \n",
    "#                              out_channels=cnn_out_channels, \n",
    "#                              kernel_size=kernel_size)\n",
    "        \n",
    "#         # LSTM layers\n",
    "#         self.lstm = nn.LSTM(input_size=cnn_out_channels, \n",
    "#                             hidden_size=hidden_size, \n",
    "#                             num_layers=num_layers, \n",
    "#                             batch_first=True)\n",
    "        \n",
    "#         # Fully connected layer for output\n",
    "#         self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.permute(0, 2, 1)\n",
    "#         x = torch.relu(self.cnn(x))\n",
    "#         x = x.permute(0, 2, 1)\n",
    "#         x, _ = self.lstm(x)\n",
    "#         x = x[:, -1, :]\n",
    "#         x = self.fc(x)\n",
    "        \n",
    "#         return x\n",
    "\n",
    "# input_size = 2608\n",
    "\n",
    "\n",
    "# sequence_length = 1  \n",
    "# num_samples = X_train.shape[0]\n",
    "# num_features = X_train.shape[1]\n",
    "# num_sequences = num_samples // sequence_length\n",
    "# X = X_train.reshape(num_sequences, num_features, sequence_length)\n",
    "# # y = y.view(-1, sequence_length)\n",
    "\n",
    "# X_data_permuted = X.permute(0, 2, 1) \n",
    "\n",
    "# print(X_data_permuted.shape)\n",
    "\n",
    "# train_dataset = TensorDataset(X_data_permuted, y[:num_sequences])  \n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "# input_size = num_features \n",
    "# hidden_size = 64\n",
    "# num_layers = 2\n",
    "# cnn_out_channels = 32\n",
    "# kernel_size = sequence_length\n",
    "# output_size = 1  \n",
    "# learning_rate = 0.001\n",
    "# num_epochs = 80\n",
    "\n",
    "# model = CNN_LSTM_Model(input_size, hidden_size, num_layers, cnn_out_channels, kernel_size, output_size)\n",
    "\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "\n",
    "#     for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         outputs = model(inputs)\n",
    "        \n",
    "#         loss = criterion(outputs.squeeze(), targets.squeeze())  \n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "\n",
    "#     print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# sequence_length = 1 \n",
    "# num_samples = test_X.shape[0]\n",
    "# num_features = test_X.shape[1]\n",
    "# num_sequences = num_samples // sequence_length\n",
    "# test_X = test_X.reshape(num_sequences, num_features, sequence_length) \n",
    "# test_X_data_permuted = test_X.permute(0, 2, 1)\n",
    "\n",
    "# test_dataset = TensorDataset(test_X_data_permuted, test_y[:num_sequences])  # Ensure y matches the number of sequences\n",
    "# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# model.eval()\n",
    "\n",
    "# all_preds = []\n",
    "# all_labels = []\n",
    "\n",
    "# with torch.no_grad():  \n",
    "#     for inputs, targets in test_loader:\n",
    "\n",
    "#         outputs = model(inputs)\n",
    "        \n",
    "#         targets = targets.squeeze()\n",
    "\n",
    "#         all_preds.extend(torch.sigmoid(outputs).squeeze().cpu().numpy())  \n",
    "#         all_labels.extend(targets.cpu().numpy()) \n",
    "\n",
    "# print(all_preds)\n",
    "# all_preds_binary = [1 if pred > 0.5 else 0 for pred in all_preds]\n",
    "\n",
    "# accuracy = accuracy_score(all_labels, all_preds_binary)\n",
    "# print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# conf_matrix = confusion_matrix(all_labels, all_preds_binary)\n",
    "# print(f\"Confusion Matrix:\\n{conf_matrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Twk23\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.7162\n",
      "Epoch 2/100, Loss: 0.7115\n",
      "Epoch 3/100, Loss: 0.7112\n",
      "Epoch 4/100, Loss: 0.7079\n",
      "Epoch 5/100, Loss: 0.7096\n",
      "Epoch 6/100, Loss: 0.7092\n",
      "Epoch 7/100, Loss: 0.7111\n",
      "Epoch 8/100, Loss: 0.6959\n",
      "Epoch 9/100, Loss: 0.6768\n",
      "Epoch 10/100, Loss: 0.6615\n",
      "Epoch 11/100, Loss: 0.6566\n",
      "Epoch 12/100, Loss: 0.6524\n",
      "Epoch 13/100, Loss: 0.6440\n",
      "Epoch 14/100, Loss: 0.6420\n",
      "Epoch 15/100, Loss: 0.6324\n",
      "Epoch 16/100, Loss: 0.6249\n",
      "Epoch 17/100, Loss: 0.6191\n",
      "Epoch 18/100, Loss: 0.6135\n",
      "Epoch 19/100, Loss: 0.6063\n",
      "Epoch 20/100, Loss: 0.6008\n",
      "Epoch 21/100, Loss: 0.5963\n",
      "Epoch 22/100, Loss: 0.5974\n",
      "Epoch 23/100, Loss: 0.5893\n",
      "Epoch 24/100, Loss: 0.5837\n",
      "Epoch 25/100, Loss: 0.5751\n",
      "Epoch 26/100, Loss: 0.5713\n",
      "Epoch 27/100, Loss: 0.5718\n",
      "Epoch 28/100, Loss: 0.5495\n",
      "Epoch 29/100, Loss: 0.5527\n",
      "Epoch 30/100, Loss: 0.5449\n",
      "Epoch 31/100, Loss: 0.5477\n",
      "Epoch 32/100, Loss: 0.5326\n",
      "Epoch 33/100, Loss: 0.5296\n",
      "Epoch 34/100, Loss: 0.5228\n",
      "Epoch 35/100, Loss: 0.5148\n",
      "Epoch 36/100, Loss: 0.5223\n",
      "Epoch 37/100, Loss: 0.4926\n",
      "Epoch 38/100, Loss: 0.5061\n",
      "Epoch 39/100, Loss: 0.5084\n",
      "Epoch 40/100, Loss: 0.4965\n",
      "Epoch 41/100, Loss: 0.4946\n",
      "Epoch 42/100, Loss: 0.4732\n",
      "Epoch 43/100, Loss: 0.4678\n",
      "Epoch 44/100, Loss: 0.4806\n",
      "Epoch 45/100, Loss: 0.4551\n",
      "Epoch 46/100, Loss: 0.4535\n",
      "Epoch 47/100, Loss: 0.4326\n",
      "Epoch 48/100, Loss: 0.4433\n",
      "Epoch 49/100, Loss: 0.4280\n",
      "Epoch 50/100, Loss: 0.4300\n",
      "Epoch 51/100, Loss: 0.4139\n",
      "Epoch 52/100, Loss: 0.4114\n",
      "Epoch 53/100, Loss: 0.3982\n",
      "Epoch 54/100, Loss: 0.4001\n",
      "Epoch 55/100, Loss: 0.3959\n",
      "Epoch 56/100, Loss: 0.3924\n",
      "Epoch 57/100, Loss: 0.3742\n",
      "Epoch 58/100, Loss: 0.4039\n",
      "Epoch 59/100, Loss: 0.3589\n",
      "Epoch 60/100, Loss: 0.3695\n",
      "Epoch 61/100, Loss: 0.3509\n",
      "Epoch 62/100, Loss: 0.3456\n",
      "Epoch 63/100, Loss: 0.3527\n",
      "Epoch 64/100, Loss: 0.3276\n",
      "Epoch 65/100, Loss: 0.3588\n",
      "Epoch 66/100, Loss: 0.3388\n",
      "Epoch 67/100, Loss: 0.3179\n",
      "Epoch 68/100, Loss: 0.3412\n",
      "Epoch 69/100, Loss: 0.2982\n",
      "Epoch 70/100, Loss: 0.2816\n",
      "Epoch 71/100, Loss: 0.3046\n",
      "Epoch 72/100, Loss: 0.2704\n",
      "Epoch 73/100, Loss: 0.2902\n",
      "Epoch 74/100, Loss: 0.2800\n",
      "Epoch 75/100, Loss: 0.2690\n",
      "Epoch 76/100, Loss: 0.2575\n",
      "Epoch 77/100, Loss: 0.2679\n",
      "Epoch 78/100, Loss: 0.2715\n",
      "Epoch 79/100, Loss: 0.2595\n",
      "Epoch 80/100, Loss: 0.2392\n",
      "Epoch 81/100, Loss: 0.2556\n",
      "Epoch 82/100, Loss: 0.2606\n",
      "Epoch 83/100, Loss: 0.2461\n",
      "Epoch 84/100, Loss: 0.2314\n",
      "Epoch 85/100, Loss: 0.2237\n",
      "Epoch 86/100, Loss: 0.2140\n",
      "Epoch 87/100, Loss: 0.2047\n",
      "Epoch 88/100, Loss: 0.2208\n",
      "Epoch 89/100, Loss: 0.2091\n",
      "Epoch 90/100, Loss: 0.2316\n",
      "Epoch 91/100, Loss: 0.2047\n",
      "Epoch 92/100, Loss: 0.1849\n",
      "Epoch 93/100, Loss: 0.2135\n",
      "Epoch 94/100, Loss: 0.1971\n",
      "Epoch 95/100, Loss: 0.2030\n",
      "Epoch 96/100, Loss: 0.1951\n",
      "Epoch 97/100, Loss: 0.2038\n",
      "Epoch 98/100, Loss: 0.1898\n",
      "Epoch 99/100, Loss: 0.1859\n",
      "Epoch 100/100, Loss: 0.1731\n"
     ]
    }
   ],
   "source": [
    "class TransformerTimeSeriesModel(nn.Module):\n",
    "    def __init__(self, input_size, d_model, nhead, num_encoder_layers, dim_feedforward, output_size, dropout=0.1):\n",
    "        super(TransformerTimeSeriesModel, self).__init__()\n",
    "        \n",
    "        self.input_fc = nn.Linear(input_size, d_model)\n",
    "        \n",
    "        self.positional_encoding = nn.Parameter(\n",
    "            torch.zeros(1000, d_model)  # Fixed size for simplicity; adjust as needed\n",
    "        )\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout),\n",
    "            num_layers=num_encoder_layers\n",
    "        )\n",
    "        \n",
    "        # Fully connected output layer\n",
    "        self.output_fc = nn.Linear(d_model, output_size)\n",
    "\n",
    "        # Optional dropout to avoid overfitting\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Project input to d_model\n",
    "        x = self.input_fc(x)  # Shape: (batch_size, seq_len, d_model)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.positional_encoding[:seq_len, :].unsqueeze(0)\n",
    "        \n",
    "        # Transpose for Transformer (seq_len, batch_size, d_model)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        \n",
    "        # Apply Transformer Encoder\n",
    "        x = self.transformer_encoder(x)  # Shape: (seq_len, batch_size, d_model)\n",
    "        \n",
    "        # Aggregate outputs (e.g., last time step or mean pooling)\n",
    "        x = x.mean(dim=0)  # Alternative: x[-1, :, :] for the last time step\n",
    "        \n",
    "        # Apply dropout\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Fully connected output layer\n",
    "        x = self.output_fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "input_size = 19       \n",
    "d_model = 64           \n",
    "nhead = 4               \n",
    "num_encoder_layers = 3  \n",
    "dim_feedforward = 128    \n",
    "output_size = 1          \n",
    "\n",
    "model = TransformerTimeSeriesModel(input_size, d_model, nhead, num_encoder_layers, dim_feedforward, output_size)\n",
    "\n",
    "\n",
    "# Calculate the imbalanced ratio\n",
    "num_positive = (y_train == 1).sum().item()\n",
    "num_negative = (y_train == 0).sum().item()\n",
    "imbalanced_ratio = num_negative / num_positive\n",
    "\n",
    "pos_weight = torch.tensor([imbalanced_ratio])\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "model.train()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()  # Zero gradients\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs.squeeze(), y_batch.squeeze())\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5860\n",
      "Confusion Matrix:\n",
      "[[299 171]\n",
      " [183 202]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = torch.round(torch.sigmoid(model(X_test)))\n",
    "\n",
    "# print(predictions)\n",
    "predictions = predictions.cpu().numpy().flatten()\n",
    "accuracy = accuracy_score(predictions, y_test)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, predictions)\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
