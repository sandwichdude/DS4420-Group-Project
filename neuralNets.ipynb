{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Twk23\\AppData\\Local\\Temp\\ipykernel_5676\\3747382305.py:16: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  y_train_score = y_train_score.replace({\"Over\": True, \"Under\": False})\n",
      "C:\\Users\\Twk23\\AppData\\Local\\Temp\\ipykernel_5676\\3747382305.py:24: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  y_train_spread = y_train_spread.replace({\"Cover\": True, \"Did Not Cover\": False})\n",
      "C:\\Users\\Twk23\\AppData\\Local\\Temp\\ipykernel_5676\\3747382305.py:36: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  y_test_score = y_test_score.replace({\"Over\": True, \"Under\": False})\n",
      "C:\\Users\\Twk23\\AppData\\Local\\Temp\\ipykernel_5676\\3747382305.py:44: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  y_test_spread = y_test_spread.replace({\"Cover\": True, \"Did Not Cover\": False})\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "scalar1 = MinMaxScaler()\n",
    "scalar2 = MinMaxScaler()\n",
    "\n",
    "X_train_score = pd.read_csv('trainX_score.csv')  \n",
    "y_train_score = pd.read_csv('trainY_score.csv')\n",
    "\n",
    "y_train_score = y_train_score.replace({\"Over\": True, \"Under\": False})\n",
    "\n",
    "X_train_score = X_train_score.to_numpy(dtype=np.float32)\n",
    "y_train_score = y_train_score.to_numpy(dtype=np.float32)\n",
    "\n",
    "X_train_spread = pd.read_csv('trainX_spread.csv')\n",
    "y_train_spread = pd.read_csv('trainY_spread.csv')\n",
    "\n",
    "y_train_spread = y_train_spread.replace({\"Cover\": True, \"Did Not Cover\": False})\n",
    "\n",
    "X_train_spread = X_train_spread.to_numpy(dtype=np.float32)\n",
    "y_train_spread = y_train_spread.to_numpy(dtype=np.float32)\n",
    "\n",
    "X_train_score = scalar1.fit_transform(X_train_score)\n",
    "X_train_spread = scalar2.fit_transform(X_train_spread)\n",
    "\n",
    "\n",
    "X_test_score = pd.read_csv('testX_score.csv')\n",
    "y_test_score = pd.read_csv('testY_score.csv')\n",
    "\n",
    "y_test_score = y_test_score.replace({\"Over\": True, \"Under\": False})\n",
    "\n",
    "X_test_score = X_test_score.to_numpy(dtype=np.float32)\n",
    "y_test_score = y_test_score.to_numpy(dtype=np.float32)\n",
    "\n",
    "X_test_spread = pd.read_csv('testX_spread.csv')\n",
    "y_test_spread = pd.read_csv('testY_spread.csv')\n",
    "\n",
    "y_test_spread = y_test_spread.replace({\"Cover\": True, \"Did Not Cover\": False})\n",
    "\n",
    "X_test_spread = X_test_spread.to_numpy(dtype=np.float32)\n",
    "y_test_spread = y_test_spread.to_numpy(dtype=np.float32)\n",
    "\n",
    "X_test_score = scalar1.transform(X_test_score)\n",
    "X_test_spread = scalar2.transform(X_test_spread)\n",
    "\n",
    "def toSequences(seq_size, columns, X, y):\n",
    "    X_seq = []\n",
    "    y_seq = []\n",
    "    for i in range(len(X) - seq_size):\n",
    "        X_seq.append(X[i:i+seq_size])\n",
    "        y_seq.append(y[i+seq_size-1]) \n",
    "    return torch.tensor(X_seq, dtype=torch.float32).view(-1, seq_size, columns), torch.tensor(y_seq, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "X_train_score, y_train_score = toSequences(20, X_train_score.shape[1], X_train_score, y_train_score)\n",
    "X_test_score, y_test_score = toSequences(20, X_test_score.shape[1], X_test_score, y_test_score)\n",
    "\n",
    "X_train_spread, y_train_spread = toSequences(20, X_train_spread.shape[1], X_train_spread, y_train_spread)\n",
    "X_test_spread, y_test_spread = toSequences(20, X_test_spread.shape[1], X_test_spread, y_test_spread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2608, 20, 19]) torch.Size([2608, 1])\n",
      "torch.Size([855, 20, 19]) torch.Size([855, 1])\n",
      "torch.Size([2599, 20, 24]) torch.Size([2599, 1])\n",
      "torch.Size([852, 20, 24]) torch.Size([852, 1])\n"
     ]
    }
   ],
   "source": [
    "print(X_train_score.shape, y_train_score.shape)\n",
    "print(X_test_score.shape, y_test_score.shape)\n",
    "print(X_train_spread.shape, y_train_spread.shape)\n",
    "print(X_test_spread.shape, y_test_spread.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CNN_LSTM_Model(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers, cnn_out_channels, kernel_size, output_size):\n",
    "#         super(CNN_LSTM_Model, self).__init__()\n",
    "        \n",
    "#         # CNN layers\n",
    "#         self.cnn = nn.Conv1d(in_channels=input_size, \n",
    "#                              out_channels=cnn_out_channels, \n",
    "#                              kernel_size=kernel_size)\n",
    "        \n",
    "#         # LSTM layers\n",
    "#         self.lstm = nn.LSTM(input_size=cnn_out_channels, \n",
    "#                             hidden_size=hidden_size, \n",
    "#                             num_layers=num_layers, \n",
    "#                             batch_first=True)\n",
    "        \n",
    "#         # Fully connected layer for output\n",
    "#         self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.permute(0, 2, 1)\n",
    "#         x = torch.relu(self.cnn(x))\n",
    "#         x = x.permute(0, 2, 1)\n",
    "#         x, _ = self.lstm(x)\n",
    "#         x = x[:, -1, :]\n",
    "#         x = self.fc(x)\n",
    "        \n",
    "#         return x\n",
    "\n",
    "# input_size = 2608\n",
    "\n",
    "\n",
    "# sequence_length = 1  \n",
    "# num_samples = X_train.shape[0]\n",
    "# num_features = X_train.shape[1]\n",
    "# num_sequences = num_samples // sequence_length\n",
    "# X = X_train.reshape(num_sequences, num_features, sequence_length)\n",
    "# # y = y.view(-1, sequence_length)\n",
    "\n",
    "# X_data_permuted = X.permute(0, 2, 1) \n",
    "\n",
    "# print(X_data_permuted.shape)\n",
    "\n",
    "# train_dataset = TensorDataset(X_data_permuted, y[:num_sequences])  \n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "# input_size = num_features \n",
    "# hidden_size = 64\n",
    "# num_layers = 2\n",
    "# cnn_out_channels = 32\n",
    "# kernel_size = sequence_length\n",
    "# output_size = 1  \n",
    "# learning_rate = 0.001\n",
    "# num_epochs = 80\n",
    "\n",
    "# model = CNN_LSTM_Model(input_size, hidden_size, num_layers, cnn_out_channels, kernel_size, output_size)\n",
    "\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "\n",
    "#     for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         outputs = model(inputs)\n",
    "        \n",
    "#         loss = criterion(outputs.squeeze(), targets.squeeze())  \n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "\n",
    "#     print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# sequence_length = 1 \n",
    "# num_samples = test_X.shape[0]\n",
    "# num_features = test_X.shape[1]\n",
    "# num_sequences = num_samples // sequence_length\n",
    "# test_X = test_X.reshape(num_sequences, num_features, sequence_length) \n",
    "# test_X_data_permuted = test_X.permute(0, 2, 1)\n",
    "\n",
    "# test_dataset = TensorDataset(test_X_data_permuted, test_y[:num_sequences])  # Ensure y matches the number of sequences\n",
    "# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# model.eval()\n",
    "\n",
    "# all_preds = []\n",
    "# all_labels = []\n",
    "\n",
    "# with torch.no_grad():  \n",
    "#     for inputs, targets in test_loader:\n",
    "\n",
    "#         outputs = model(inputs)\n",
    "        \n",
    "#         targets = targets.squeeze()\n",
    "\n",
    "#         all_preds.extend(torch.sigmoid(outputs).squeeze().cpu().numpy())  \n",
    "#         all_labels.extend(targets.cpu().numpy()) \n",
    "\n",
    "# print(all_preds)\n",
    "# all_preds_binary = [1 if pred > 0.5 else 0 for pred in all_preds]\n",
    "\n",
    "# accuracy = accuracy_score(all_labels, all_preds_binary)\n",
    "# print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# conf_matrix = confusion_matrix(all_labels, all_preds_binary)\n",
    "# print(f\"Confusion Matrix:\\n{conf_matrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Twk23\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, Loss: 0.7239\n",
      "Epoch 2/300, Loss: 0.7097\n",
      "Epoch 3/300, Loss: 0.7118\n",
      "Epoch 4/300, Loss: 0.7096\n",
      "Epoch 5/300, Loss: 0.7095\n",
      "Epoch 6/300, Loss: 0.7084\n",
      "Epoch 7/300, Loss: 0.6903\n",
      "Epoch 8/300, Loss: 0.6715\n",
      "Epoch 9/300, Loss: 0.6559\n",
      "Epoch 10/300, Loss: 0.6485\n",
      "Epoch 11/300, Loss: 0.6496\n",
      "Epoch 12/300, Loss: 0.6388\n",
      "Epoch 13/300, Loss: 0.6308\n",
      "Epoch 14/300, Loss: 0.6244\n",
      "Epoch 15/300, Loss: 0.6141\n",
      "Epoch 16/300, Loss: 0.6215\n",
      "Epoch 17/300, Loss: 0.6156\n",
      "Epoch 18/300, Loss: 0.6056\n",
      "Epoch 19/300, Loss: 0.6201\n",
      "Epoch 20/300, Loss: 0.5961\n",
      "Epoch 21/300, Loss: 0.6020\n",
      "Epoch 22/300, Loss: 0.5916\n",
      "Epoch 23/300, Loss: 0.5863\n",
      "Epoch 24/300, Loss: 0.5830\n",
      "Epoch 25/300, Loss: 0.5689\n",
      "Epoch 26/300, Loss: 0.5692\n",
      "Epoch 27/300, Loss: 0.5701\n",
      "Epoch 28/300, Loss: 0.5481\n",
      "Epoch 29/300, Loss: 0.5635\n",
      "Epoch 30/300, Loss: 0.5531\n",
      "Epoch 31/300, Loss: 0.5369\n",
      "Epoch 32/300, Loss: 0.5418\n",
      "Epoch 33/300, Loss: 0.5286\n",
      "Epoch 34/300, Loss: 0.5191\n",
      "Epoch 35/300, Loss: 0.5112\n",
      "Epoch 36/300, Loss: 0.5096\n",
      "Epoch 37/300, Loss: 0.5089\n",
      "Epoch 38/300, Loss: 0.5165\n",
      "Epoch 39/300, Loss: 0.4937\n",
      "Epoch 40/300, Loss: 0.4754\n",
      "Epoch 41/300, Loss: 0.4660\n",
      "Epoch 42/300, Loss: 0.4668\n",
      "Epoch 43/300, Loss: 0.4472\n",
      "Epoch 44/300, Loss: 0.4496\n",
      "Epoch 45/300, Loss: 0.4334\n",
      "Epoch 46/300, Loss: 0.4245\n",
      "Epoch 47/300, Loss: 0.4148\n",
      "Epoch 48/300, Loss: 0.4226\n",
      "Epoch 49/300, Loss: 0.4006\n",
      "Epoch 50/300, Loss: 0.3943\n",
      "Epoch 51/300, Loss: 0.3902\n",
      "Epoch 52/300, Loss: 0.3962\n",
      "Epoch 53/300, Loss: 0.3746\n",
      "Epoch 54/300, Loss: 0.3567\n",
      "Epoch 55/300, Loss: 0.3641\n",
      "Epoch 56/300, Loss: 0.3547\n",
      "Epoch 57/300, Loss: 0.3543\n",
      "Epoch 58/300, Loss: 0.3355\n",
      "Epoch 59/300, Loss: 0.3382\n",
      "Epoch 60/300, Loss: 0.3219\n",
      "Epoch 61/300, Loss: 0.3167\n",
      "Epoch 62/300, Loss: 0.2876\n",
      "Epoch 63/300, Loss: 0.2886\n",
      "Epoch 64/300, Loss: 0.2981\n",
      "Epoch 65/300, Loss: 0.2962\n",
      "Epoch 66/300, Loss: 0.3007\n",
      "Epoch 67/300, Loss: 0.2660\n",
      "Epoch 68/300, Loss: 0.2659\n",
      "Epoch 69/300, Loss: 0.2627\n",
      "Epoch 70/300, Loss: 0.2597\n",
      "Epoch 71/300, Loss: 0.2461\n",
      "Epoch 72/300, Loss: 0.2347\n",
      "Epoch 73/300, Loss: 0.2285\n",
      "Epoch 74/300, Loss: 0.2235\n",
      "Epoch 75/300, Loss: 0.2266\n",
      "Epoch 76/300, Loss: 0.2121\n",
      "Epoch 77/300, Loss: 0.2059\n",
      "Epoch 78/300, Loss: 0.2054\n",
      "Epoch 79/300, Loss: 0.2205\n",
      "Epoch 80/300, Loss: 0.1863\n",
      "Epoch 81/300, Loss: 0.2159\n",
      "Epoch 82/300, Loss: 0.1939\n",
      "Epoch 83/300, Loss: 0.1744\n",
      "Epoch 84/300, Loss: 0.1840\n",
      "Epoch 85/300, Loss: 0.1779\n",
      "Epoch 86/300, Loss: 0.1663\n",
      "Epoch 87/300, Loss: 0.1657\n",
      "Epoch 88/300, Loss: 0.1535\n",
      "Epoch 89/300, Loss: 0.1613\n",
      "Epoch 90/300, Loss: 0.1399\n",
      "Epoch 91/300, Loss: 0.1783\n",
      "Epoch 92/300, Loss: 0.1544\n",
      "Epoch 93/300, Loss: 0.1322\n",
      "Epoch 94/300, Loss: 0.1475\n",
      "Epoch 95/300, Loss: 0.1570\n",
      "Epoch 96/300, Loss: 0.1289\n",
      "Epoch 97/300, Loss: 0.1617\n",
      "Epoch 98/300, Loss: 0.1140\n",
      "Epoch 99/300, Loss: 0.1243\n",
      "Epoch 100/300, Loss: 0.1555\n",
      "Epoch 101/300, Loss: 0.1246\n",
      "Epoch 102/300, Loss: 0.1256\n",
      "Epoch 103/300, Loss: 0.1481\n",
      "Epoch 104/300, Loss: 0.1366\n",
      "Epoch 105/300, Loss: 0.1068\n",
      "Epoch 106/300, Loss: 0.1156\n",
      "Epoch 107/300, Loss: 0.1255\n",
      "Epoch 108/300, Loss: 0.1197\n",
      "Epoch 109/300, Loss: 0.1083\n",
      "Epoch 110/300, Loss: 0.1084\n",
      "Epoch 111/300, Loss: 0.1112\n",
      "Epoch 112/300, Loss: 0.1062\n",
      "Epoch 113/300, Loss: 0.0888\n",
      "Epoch 114/300, Loss: 0.1021\n",
      "Epoch 115/300, Loss: 0.1137\n",
      "Epoch 116/300, Loss: 0.1063\n",
      "Epoch 117/300, Loss: 0.1095\n",
      "Epoch 118/300, Loss: 0.1095\n",
      "Epoch 119/300, Loss: 0.1230\n",
      "Epoch 120/300, Loss: 0.1038\n",
      "Epoch 121/300, Loss: 0.0948\n",
      "Epoch 122/300, Loss: 0.1008\n",
      "Epoch 123/300, Loss: 0.0940\n",
      "Epoch 124/300, Loss: 0.0945\n",
      "Epoch 125/300, Loss: 0.0989\n",
      "Epoch 126/300, Loss: 0.0990\n",
      "Epoch 127/300, Loss: 0.0807\n",
      "Epoch 128/300, Loss: 0.0628\n",
      "Epoch 129/300, Loss: 0.0979\n",
      "Epoch 130/300, Loss: 0.1135\n",
      "Epoch 131/300, Loss: 0.1005\n",
      "Epoch 132/300, Loss: 0.0787\n",
      "Epoch 133/300, Loss: 0.1168\n",
      "Epoch 134/300, Loss: 0.0859\n",
      "Epoch 135/300, Loss: 0.0849\n",
      "Epoch 136/300, Loss: 0.0985\n",
      "Epoch 137/300, Loss: 0.0795\n",
      "Epoch 138/300, Loss: 0.0831\n",
      "Epoch 139/300, Loss: 0.0894\n",
      "Epoch 140/300, Loss: 0.1030\n",
      "Epoch 141/300, Loss: 0.0760\n",
      "Epoch 142/300, Loss: 0.0929\n",
      "Epoch 143/300, Loss: 0.0918\n",
      "Epoch 144/300, Loss: 0.1185\n",
      "Epoch 145/300, Loss: 0.0898\n",
      "Epoch 146/300, Loss: 0.0765\n",
      "Epoch 147/300, Loss: 0.0745\n",
      "Epoch 148/300, Loss: 0.0900\n",
      "Epoch 149/300, Loss: 0.0759\n",
      "Epoch 150/300, Loss: 0.0671\n",
      "Epoch 151/300, Loss: 0.0853\n",
      "Epoch 152/300, Loss: 0.0881\n",
      "Epoch 153/300, Loss: 0.0785\n",
      "Epoch 154/300, Loss: 0.0833\n",
      "Epoch 155/300, Loss: 0.0681\n",
      "Epoch 156/300, Loss: 0.0627\n",
      "Epoch 157/300, Loss: 0.0632\n",
      "Epoch 158/300, Loss: 0.0924\n",
      "Epoch 159/300, Loss: 0.0698\n",
      "Epoch 160/300, Loss: 0.0717\n",
      "Epoch 161/300, Loss: 0.0610\n",
      "Epoch 162/300, Loss: 0.1073\n",
      "Epoch 163/300, Loss: 0.0670\n",
      "Epoch 164/300, Loss: 0.0874\n",
      "Epoch 165/300, Loss: 0.0679\n",
      "Epoch 166/300, Loss: 0.0644\n",
      "Epoch 167/300, Loss: 0.0716\n",
      "Epoch 168/300, Loss: 0.0942\n",
      "Epoch 169/300, Loss: 0.0834\n",
      "Epoch 170/300, Loss: 0.0553\n",
      "Epoch 171/300, Loss: 0.0553\n",
      "Epoch 172/300, Loss: 0.0913\n",
      "Epoch 173/300, Loss: 0.0740\n",
      "Epoch 174/300, Loss: 0.0685\n",
      "Epoch 175/300, Loss: 0.0781\n",
      "Epoch 176/300, Loss: 0.0721\n",
      "Epoch 177/300, Loss: 0.0461\n",
      "Epoch 178/300, Loss: 0.0896\n",
      "Epoch 179/300, Loss: 0.0922\n",
      "Epoch 180/300, Loss: 0.0752\n",
      "Epoch 181/300, Loss: 0.0710\n",
      "Epoch 182/300, Loss: 0.0736\n",
      "Epoch 183/300, Loss: 0.0605\n",
      "Epoch 184/300, Loss: 0.0514\n",
      "Epoch 185/300, Loss: 0.0755\n",
      "Epoch 186/300, Loss: 0.0517\n",
      "Epoch 187/300, Loss: 0.0820\n",
      "Epoch 188/300, Loss: 0.0718\n",
      "Epoch 189/300, Loss: 0.0533\n",
      "Epoch 190/300, Loss: 0.0530\n",
      "Epoch 191/300, Loss: 0.0606\n",
      "Epoch 192/300, Loss: 0.0602\n",
      "Epoch 193/300, Loss: 0.0571\n",
      "Epoch 194/300, Loss: 0.0586\n",
      "Epoch 195/300, Loss: 0.0771\n",
      "Epoch 196/300, Loss: 0.0675\n",
      "Epoch 197/300, Loss: 0.0551\n",
      "Epoch 198/300, Loss: 0.0749\n",
      "Epoch 199/300, Loss: 0.0696\n",
      "Epoch 200/300, Loss: 0.0508\n",
      "Epoch 201/300, Loss: 0.0568\n",
      "Epoch 202/300, Loss: 0.0998\n",
      "Epoch 203/300, Loss: 0.0749\n",
      "Epoch 204/300, Loss: 0.0718\n",
      "Epoch 205/300, Loss: 0.0569\n",
      "Epoch 206/300, Loss: 0.0746\n",
      "Epoch 207/300, Loss: 0.0692\n",
      "Epoch 208/300, Loss: 0.0624\n",
      "Epoch 209/300, Loss: 0.0633\n",
      "Epoch 210/300, Loss: 0.0517\n",
      "Epoch 211/300, Loss: 0.0787\n",
      "Epoch 212/300, Loss: 0.0524\n",
      "Epoch 213/300, Loss: 0.0509\n",
      "Epoch 214/300, Loss: 0.0605\n",
      "Epoch 215/300, Loss: 0.0687\n",
      "Epoch 216/300, Loss: 0.0675\n",
      "Epoch 217/300, Loss: 0.0692\n",
      "Epoch 218/300, Loss: 0.0497\n",
      "Epoch 219/300, Loss: 0.0736\n",
      "Epoch 220/300, Loss: 0.0596\n",
      "Epoch 221/300, Loss: 0.0766\n",
      "Epoch 222/300, Loss: 0.0434\n",
      "Epoch 223/300, Loss: 0.0600\n",
      "Epoch 224/300, Loss: 0.0528\n",
      "Epoch 225/300, Loss: 0.0662\n",
      "Epoch 226/300, Loss: 0.0486\n",
      "Epoch 227/300, Loss: 0.0577\n",
      "Epoch 228/300, Loss: 0.0718\n",
      "Epoch 229/300, Loss: 0.0497\n",
      "Epoch 230/300, Loss: 0.0602\n",
      "Epoch 231/300, Loss: 0.0671\n",
      "Epoch 232/300, Loss: 0.0549\n",
      "Epoch 233/300, Loss: 0.0647\n",
      "Epoch 234/300, Loss: 0.0757\n",
      "Epoch 235/300, Loss: 0.0788\n",
      "Epoch 236/300, Loss: 0.0560\n",
      "Epoch 237/300, Loss: 0.0755\n",
      "Epoch 238/300, Loss: 0.0457\n",
      "Epoch 239/300, Loss: 0.0654\n",
      "Epoch 240/300, Loss: 0.0718\n",
      "Epoch 241/300, Loss: 0.0589\n",
      "Epoch 242/300, Loss: 0.0546\n",
      "Epoch 243/300, Loss: 0.0554\n",
      "Epoch 244/300, Loss: 0.0391\n",
      "Epoch 245/300, Loss: 0.0545\n",
      "Epoch 246/300, Loss: 0.0747\n",
      "Epoch 247/300, Loss: 0.0327\n",
      "Epoch 248/300, Loss: 0.0449\n",
      "Epoch 249/300, Loss: 0.0554\n",
      "Epoch 250/300, Loss: 0.0510\n",
      "Epoch 251/300, Loss: 0.0559\n",
      "Epoch 252/300, Loss: 0.0580\n",
      "Epoch 253/300, Loss: 0.0461\n",
      "Epoch 254/300, Loss: 0.0533\n",
      "Epoch 255/300, Loss: 0.0378\n",
      "Epoch 256/300, Loss: 0.0423\n",
      "Epoch 257/300, Loss: 0.0602\n",
      "Epoch 258/300, Loss: 0.0550\n",
      "Epoch 259/300, Loss: 0.0628\n",
      "Epoch 260/300, Loss: 0.0568\n",
      "Epoch 261/300, Loss: 0.0496\n",
      "Epoch 262/300, Loss: 0.0290\n",
      "Epoch 263/300, Loss: 0.0829\n",
      "Epoch 264/300, Loss: 0.0388\n",
      "Epoch 265/300, Loss: 0.0491\n",
      "Epoch 266/300, Loss: 0.0848\n",
      "Epoch 267/300, Loss: 0.0554\n",
      "Epoch 268/300, Loss: 0.0462\n",
      "Epoch 269/300, Loss: 0.0516\n",
      "Epoch 270/300, Loss: 0.0488\n",
      "Epoch 271/300, Loss: 0.0408\n",
      "Epoch 272/300, Loss: 0.0439\n",
      "Epoch 273/300, Loss: 0.0449\n",
      "Epoch 274/300, Loss: 0.0385\n",
      "Epoch 275/300, Loss: 0.0585\n",
      "Epoch 276/300, Loss: 0.0423\n",
      "Epoch 277/300, Loss: 0.0614\n",
      "Epoch 278/300, Loss: 0.0600\n",
      "Epoch 279/300, Loss: 0.0565\n",
      "Epoch 280/300, Loss: 0.0347\n",
      "Epoch 281/300, Loss: 0.0491\n",
      "Epoch 282/300, Loss: 0.0541\n",
      "Epoch 283/300, Loss: 0.0506\n",
      "Epoch 284/300, Loss: 0.0471\n",
      "Epoch 285/300, Loss: 0.0460\n",
      "Epoch 286/300, Loss: 0.0428\n",
      "Epoch 287/300, Loss: 0.0508\n",
      "Epoch 288/300, Loss: 0.0511\n",
      "Epoch 289/300, Loss: 0.0365\n",
      "Epoch 290/300, Loss: 0.0533\n",
      "Epoch 291/300, Loss: 0.0618\n",
      "Epoch 292/300, Loss: 0.0504\n",
      "Epoch 293/300, Loss: 0.0577\n",
      "Epoch 294/300, Loss: 0.0424\n",
      "Epoch 295/300, Loss: 0.0452\n",
      "Epoch 296/300, Loss: 0.0451\n",
      "Epoch 297/300, Loss: 0.0670\n",
      "Epoch 298/300, Loss: 0.0572\n",
      "Epoch 299/300, Loss: 0.0552\n",
      "Epoch 300/300, Loss: 0.0404\n",
      "Epoch 1/300, Loss: 0.7540\n",
      "Epoch 2/300, Loss: 0.7461\n",
      "Epoch 3/300, Loss: 0.7424\n",
      "Epoch 4/300, Loss: 0.7437\n",
      "Epoch 5/300, Loss: 0.7391\n",
      "Epoch 6/300, Loss: 0.7420\n",
      "Epoch 7/300, Loss: 0.7402\n",
      "Epoch 8/300, Loss: 0.7397\n",
      "Epoch 9/300, Loss: 0.7379\n",
      "Epoch 10/300, Loss: 0.7394\n",
      "Epoch 11/300, Loss: 0.7384\n",
      "Epoch 12/300, Loss: 0.7391\n",
      "Epoch 13/300, Loss: 0.7381\n",
      "Epoch 14/300, Loss: 0.7390\n",
      "Epoch 15/300, Loss: 0.7387\n",
      "Epoch 16/300, Loss: 0.7356\n",
      "Epoch 17/300, Loss: 0.7362\n",
      "Epoch 18/300, Loss: 0.7363\n",
      "Epoch 19/300, Loss: 0.7309\n",
      "Epoch 20/300, Loss: 0.7264\n",
      "Epoch 21/300, Loss: 0.7288\n",
      "Epoch 22/300, Loss: 0.7202\n",
      "Epoch 23/300, Loss: 0.7087\n",
      "Epoch 24/300, Loss: 0.6993\n",
      "Epoch 25/300, Loss: 0.6893\n",
      "Epoch 26/300, Loss: 0.6832\n",
      "Epoch 27/300, Loss: 0.6816\n",
      "Epoch 28/300, Loss: 0.6756\n",
      "Epoch 29/300, Loss: 0.6728\n",
      "Epoch 30/300, Loss: 0.6587\n",
      "Epoch 31/300, Loss: 0.6639\n",
      "Epoch 32/300, Loss: 0.6540\n",
      "Epoch 33/300, Loss: 0.6527\n",
      "Epoch 34/300, Loss: 0.6438\n",
      "Epoch 35/300, Loss: 0.6456\n",
      "Epoch 36/300, Loss: 0.6373\n",
      "Epoch 37/300, Loss: 0.6340\n",
      "Epoch 38/300, Loss: 0.6183\n",
      "Epoch 39/300, Loss: 0.6292\n",
      "Epoch 40/300, Loss: 0.6164\n",
      "Epoch 41/300, Loss: 0.6073\n",
      "Epoch 42/300, Loss: 0.6074\n",
      "Epoch 43/300, Loss: 0.5993\n",
      "Epoch 44/300, Loss: 0.5905\n",
      "Epoch 45/300, Loss: 0.5890\n",
      "Epoch 46/300, Loss: 0.5745\n",
      "Epoch 47/300, Loss: 0.5660\n",
      "Epoch 48/300, Loss: 0.5471\n",
      "Epoch 49/300, Loss: 0.5454\n",
      "Epoch 50/300, Loss: 0.5264\n",
      "Epoch 51/300, Loss: 0.5300\n",
      "Epoch 52/300, Loss: 0.5189\n",
      "Epoch 53/300, Loss: 0.5201\n",
      "Epoch 54/300, Loss: 0.4965\n",
      "Epoch 55/300, Loss: 0.4856\n",
      "Epoch 56/300, Loss: 0.4965\n",
      "Epoch 57/300, Loss: 0.4747\n",
      "Epoch 58/300, Loss: 0.4886\n",
      "Epoch 59/300, Loss: 0.4667\n",
      "Epoch 60/300, Loss: 0.4467\n",
      "Epoch 61/300, Loss: 0.4441\n",
      "Epoch 62/300, Loss: 0.4439\n",
      "Epoch 63/300, Loss: 0.4194\n",
      "Epoch 64/300, Loss: 0.4181\n",
      "Epoch 65/300, Loss: 0.4152\n",
      "Epoch 66/300, Loss: 0.4124\n",
      "Epoch 67/300, Loss: 0.4030\n",
      "Epoch 68/300, Loss: 0.3902\n",
      "Epoch 69/300, Loss: 0.3640\n",
      "Epoch 70/300, Loss: 0.3857\n",
      "Epoch 71/300, Loss: 0.3698\n",
      "Epoch 72/300, Loss: 0.3512\n",
      "Epoch 73/300, Loss: 0.3254\n",
      "Epoch 74/300, Loss: 0.3613\n",
      "Epoch 75/300, Loss: 0.3319\n",
      "Epoch 76/300, Loss: 0.3047\n",
      "Epoch 77/300, Loss: 0.3404\n",
      "Epoch 78/300, Loss: 0.3377\n",
      "Epoch 79/300, Loss: 0.3187\n",
      "Epoch 80/300, Loss: 0.3028\n",
      "Epoch 81/300, Loss: 0.2877\n",
      "Epoch 82/300, Loss: 0.2714\n",
      "Epoch 83/300, Loss: 0.2700\n",
      "Epoch 84/300, Loss: 0.2890\n",
      "Epoch 85/300, Loss: 0.2688\n",
      "Epoch 86/300, Loss: 0.2530\n",
      "Epoch 87/300, Loss: 0.2559\n",
      "Epoch 88/300, Loss: 0.2590\n",
      "Epoch 89/300, Loss: 0.2508\n",
      "Epoch 90/300, Loss: 0.2562\n",
      "Epoch 91/300, Loss: 0.2315\n",
      "Epoch 92/300, Loss: 0.2196\n",
      "Epoch 93/300, Loss: 0.2321\n",
      "Epoch 94/300, Loss: 0.2288\n",
      "Epoch 95/300, Loss: 0.2052\n",
      "Epoch 96/300, Loss: 0.2205\n",
      "Epoch 97/300, Loss: 0.2108\n",
      "Epoch 98/300, Loss: 0.1945\n",
      "Epoch 99/300, Loss: 0.2096\n",
      "Epoch 100/300, Loss: 0.2286\n",
      "Epoch 101/300, Loss: 0.1946\n",
      "Epoch 102/300, Loss: 0.1857\n",
      "Epoch 103/300, Loss: 0.2124\n",
      "Epoch 104/300, Loss: 0.1934\n",
      "Epoch 105/300, Loss: 0.1989\n",
      "Epoch 106/300, Loss: 0.2185\n",
      "Epoch 107/300, Loss: 0.1892\n",
      "Epoch 108/300, Loss: 0.1903\n",
      "Epoch 109/300, Loss: 0.2106\n",
      "Epoch 110/300, Loss: 0.1715\n",
      "Epoch 111/300, Loss: 0.1811\n",
      "Epoch 112/300, Loss: 0.1617\n",
      "Epoch 113/300, Loss: 0.1519\n",
      "Epoch 114/300, Loss: 0.1922\n",
      "Epoch 115/300, Loss: 0.1818\n",
      "Epoch 116/300, Loss: 0.1504\n",
      "Epoch 117/300, Loss: 0.1655\n",
      "Epoch 118/300, Loss: 0.1375\n",
      "Epoch 119/300, Loss: 0.1308\n",
      "Epoch 120/300, Loss: 0.1734\n",
      "Epoch 121/300, Loss: 0.1469\n",
      "Epoch 122/300, Loss: 0.1517\n",
      "Epoch 123/300, Loss: 0.1483\n",
      "Epoch 124/300, Loss: 0.1439\n",
      "Epoch 125/300, Loss: 0.1393\n",
      "Epoch 126/300, Loss: 0.1502\n",
      "Epoch 127/300, Loss: 0.1026\n",
      "Epoch 128/300, Loss: 0.1212\n",
      "Epoch 129/300, Loss: 0.1677\n",
      "Epoch 130/300, Loss: 0.1454\n",
      "Epoch 131/300, Loss: 0.1444\n",
      "Epoch 132/300, Loss: 0.1509\n",
      "Epoch 133/300, Loss: 0.1284\n",
      "Epoch 134/300, Loss: 0.1357\n",
      "Epoch 135/300, Loss: 0.1297\n",
      "Epoch 136/300, Loss: 0.1293\n",
      "Epoch 137/300, Loss: 0.1349\n",
      "Epoch 138/300, Loss: 0.1155\n",
      "Epoch 139/300, Loss: 0.1403\n",
      "Epoch 140/300, Loss: 0.1330\n",
      "Epoch 141/300, Loss: 0.1046\n",
      "Epoch 142/300, Loss: 0.1115\n",
      "Epoch 143/300, Loss: 0.1029\n",
      "Epoch 144/300, Loss: 0.1045\n",
      "Epoch 145/300, Loss: 0.1164\n",
      "Epoch 146/300, Loss: 0.1174\n",
      "Epoch 147/300, Loss: 0.1218\n",
      "Epoch 148/300, Loss: 0.1070\n",
      "Epoch 149/300, Loss: 0.1111\n",
      "Epoch 150/300, Loss: 0.1364\n",
      "Epoch 151/300, Loss: 0.1136\n",
      "Epoch 152/300, Loss: 0.1139\n",
      "Epoch 153/300, Loss: 0.1429\n",
      "Epoch 154/300, Loss: 0.1129\n",
      "Epoch 155/300, Loss: 0.1076\n",
      "Epoch 156/300, Loss: 0.1037\n",
      "Epoch 157/300, Loss: 0.1063\n",
      "Epoch 158/300, Loss: 0.1163\n",
      "Epoch 159/300, Loss: 0.0914\n",
      "Epoch 160/300, Loss: 0.1023\n",
      "Epoch 161/300, Loss: 0.1052\n",
      "Epoch 162/300, Loss: 0.0947\n",
      "Epoch 163/300, Loss: 0.1017\n",
      "Epoch 164/300, Loss: 0.0955\n",
      "Epoch 165/300, Loss: 0.1149\n",
      "Epoch 166/300, Loss: 0.0937\n",
      "Epoch 167/300, Loss: 0.0957\n",
      "Epoch 168/300, Loss: 0.1192\n",
      "Epoch 169/300, Loss: 0.0858\n",
      "Epoch 170/300, Loss: 0.0960\n",
      "Epoch 171/300, Loss: 0.0977\n",
      "Epoch 172/300, Loss: 0.0883\n",
      "Epoch 173/300, Loss: 0.0972\n",
      "Epoch 174/300, Loss: 0.1179\n",
      "Epoch 175/300, Loss: 0.1102\n",
      "Epoch 176/300, Loss: 0.0923\n",
      "Epoch 177/300, Loss: 0.0892\n",
      "Epoch 178/300, Loss: 0.1039\n",
      "Epoch 179/300, Loss: 0.1223\n",
      "Epoch 180/300, Loss: 0.1006\n",
      "Epoch 181/300, Loss: 0.0883\n",
      "Epoch 182/300, Loss: 0.0977\n",
      "Epoch 183/300, Loss: 0.1028\n",
      "Epoch 184/300, Loss: 0.1191\n",
      "Epoch 185/300, Loss: 0.0931\n",
      "Epoch 186/300, Loss: 0.0938\n",
      "Epoch 187/300, Loss: 0.0944\n",
      "Epoch 188/300, Loss: 0.0994\n",
      "Epoch 189/300, Loss: 0.0795\n",
      "Epoch 190/300, Loss: 0.0868\n",
      "Epoch 191/300, Loss: 0.0887\n",
      "Epoch 192/300, Loss: 0.0968\n",
      "Epoch 193/300, Loss: 0.0790\n",
      "Epoch 194/300, Loss: 0.1187\n",
      "Epoch 195/300, Loss: 0.0894\n",
      "Epoch 196/300, Loss: 0.1037\n",
      "Epoch 197/300, Loss: 0.0985\n",
      "Epoch 198/300, Loss: 0.0770\n",
      "Epoch 199/300, Loss: 0.0838\n",
      "Epoch 200/300, Loss: 0.0983\n",
      "Epoch 201/300, Loss: 0.0976\n",
      "Epoch 202/300, Loss: 0.0818\n",
      "Epoch 203/300, Loss: 0.0732\n",
      "Epoch 204/300, Loss: 0.0845\n",
      "Epoch 205/300, Loss: 0.1115\n",
      "Epoch 206/300, Loss: 0.0849\n",
      "Epoch 207/300, Loss: 0.0796\n",
      "Epoch 208/300, Loss: 0.0731\n",
      "Epoch 209/300, Loss: 0.0740\n",
      "Epoch 210/300, Loss: 0.0833\n",
      "Epoch 211/300, Loss: 0.0803\n",
      "Epoch 212/300, Loss: 0.0758\n",
      "Epoch 213/300, Loss: 0.0882\n",
      "Epoch 214/300, Loss: 0.0898\n",
      "Epoch 215/300, Loss: 0.0630\n",
      "Epoch 216/300, Loss: 0.1164\n",
      "Epoch 217/300, Loss: 0.0847\n",
      "Epoch 218/300, Loss: 0.0704\n",
      "Epoch 219/300, Loss: 0.0791\n",
      "Epoch 220/300, Loss: 0.0792\n",
      "Epoch 221/300, Loss: 0.0714\n",
      "Epoch 222/300, Loss: 0.0825\n",
      "Epoch 223/300, Loss: 0.0826\n",
      "Epoch 224/300, Loss: 0.0722\n",
      "Epoch 225/300, Loss: 0.0774\n",
      "Epoch 226/300, Loss: 0.0726\n",
      "Epoch 227/300, Loss: 0.0849\n",
      "Epoch 228/300, Loss: 0.0727\n",
      "Epoch 229/300, Loss: 0.0764\n",
      "Epoch 230/300, Loss: 0.0704\n",
      "Epoch 231/300, Loss: 0.0895\n",
      "Epoch 232/300, Loss: 0.0607\n",
      "Epoch 233/300, Loss: 0.1060\n",
      "Epoch 234/300, Loss: 0.0813\n",
      "Epoch 235/300, Loss: 0.0853\n",
      "Epoch 236/300, Loss: 0.0968\n",
      "Epoch 237/300, Loss: 0.0985\n",
      "Epoch 238/300, Loss: 0.0684\n",
      "Epoch 239/300, Loss: 0.0867\n",
      "Epoch 240/300, Loss: 0.0683\n",
      "Epoch 241/300, Loss: 0.1178\n",
      "Epoch 242/300, Loss: 0.0726\n",
      "Epoch 243/300, Loss: 0.0671\n",
      "Epoch 244/300, Loss: 0.1010\n",
      "Epoch 245/300, Loss: 0.0761\n",
      "Epoch 246/300, Loss: 0.0706\n",
      "Epoch 247/300, Loss: 0.0745\n",
      "Epoch 248/300, Loss: 0.0726\n",
      "Epoch 249/300, Loss: 0.0646\n",
      "Epoch 250/300, Loss: 0.0678\n",
      "Epoch 251/300, Loss: 0.0475\n",
      "Epoch 252/300, Loss: 0.0671\n",
      "Epoch 253/300, Loss: 0.0688\n",
      "Epoch 254/300, Loss: 0.0972\n",
      "Epoch 255/300, Loss: 0.0669\n",
      "Epoch 256/300, Loss: 0.0698\n",
      "Epoch 257/300, Loss: 0.0643\n",
      "Epoch 258/300, Loss: 0.0821\n",
      "Epoch 259/300, Loss: 0.0530\n",
      "Epoch 260/300, Loss: 0.0832\n",
      "Epoch 261/300, Loss: 0.0673\n",
      "Epoch 262/300, Loss: 0.0618\n",
      "Epoch 263/300, Loss: 0.0910\n",
      "Epoch 264/300, Loss: 0.0807\n",
      "Epoch 265/300, Loss: 0.0835\n",
      "Epoch 266/300, Loss: 0.0590\n",
      "Epoch 267/300, Loss: 0.0830\n",
      "Epoch 268/300, Loss: 0.0561\n",
      "Epoch 269/300, Loss: 0.0804\n",
      "Epoch 270/300, Loss: 0.0636\n",
      "Epoch 271/300, Loss: 0.0627\n",
      "Epoch 272/300, Loss: 0.0590\n",
      "Epoch 273/300, Loss: 0.0457\n",
      "Epoch 274/300, Loss: 0.0668\n",
      "Epoch 275/300, Loss: 0.0497\n",
      "Epoch 276/300, Loss: 0.0554\n",
      "Epoch 277/300, Loss: 0.0811\n",
      "Epoch 278/300, Loss: 0.0624\n",
      "Epoch 279/300, Loss: 0.0754\n",
      "Epoch 280/300, Loss: 0.1009\n",
      "Epoch 281/300, Loss: 0.0743\n",
      "Epoch 282/300, Loss: 0.0700\n",
      "Epoch 283/300, Loss: 0.0781\n",
      "Epoch 284/300, Loss: 0.0454\n",
      "Epoch 285/300, Loss: 0.0772\n",
      "Epoch 286/300, Loss: 0.0605\n",
      "Epoch 287/300, Loss: 0.0492\n",
      "Epoch 288/300, Loss: 0.0511\n",
      "Epoch 289/300, Loss: 0.0646\n",
      "Epoch 290/300, Loss: 0.0643\n",
      "Epoch 291/300, Loss: 0.0572\n",
      "Epoch 292/300, Loss: 0.0838\n",
      "Epoch 293/300, Loss: 0.0482\n",
      "Epoch 294/300, Loss: 0.0521\n",
      "Epoch 295/300, Loss: 0.0711\n",
      "Epoch 296/300, Loss: 0.0729\n",
      "Epoch 297/300, Loss: 0.0594\n",
      "Epoch 298/300, Loss: 0.0634\n",
      "Epoch 299/300, Loss: 0.0790\n",
      "Epoch 300/300, Loss: 0.0738\n"
     ]
    }
   ],
   "source": [
    "class TransformerTimeSeriesModel(nn.Module):\n",
    "    def __init__(self, input_size, d_model, nhead, num_encoder_layers, dim_feedforward, output_size, dropout=0.1):\n",
    "        super(TransformerTimeSeriesModel, self).__init__()\n",
    "        \n",
    "        self.input_fc = nn.Linear(input_size, d_model)\n",
    "        self.positional_encoding = nn.Parameter(\n",
    "            torch.zeros(1000, d_model) \n",
    "        )\n",
    "        # Transformer Encoder\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout),\n",
    "            num_layers=num_encoder_layers\n",
    "        )\n",
    "        # Fully connected output layer\n",
    "        self.output_fc = nn.Linear(d_model, output_size)\n",
    "        # Optional dropout to avoid overfitting\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_fc(x)  \n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.positional_encoding[:seq_len, :].unsqueeze(0)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.transformer_encoder(x)  \n",
    "        x = x.mean(dim=0) \n",
    "        x = self.dropout(x)\n",
    "        x = self.output_fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "train_dataset_score = TensorDataset(X_train_score, y_train_score)\n",
    "train_loader_score = DataLoader(train_dataset_score, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset_score = TensorDataset(X_test_score, y_test_score)\n",
    "test_loader_score = DataLoader(test_dataset_score, batch_size=32, shuffle=False)\n",
    "\n",
    "train_dataset_spread = TensorDataset(X_train_spread, y_train_spread)\n",
    "train_loader_spread = DataLoader(train_dataset_spread, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset_spread = TensorDataset(X_test_spread, y_test_spread)\n",
    "test_loader_spread = DataLoader(test_dataset_spread, batch_size=32, shuffle=False)\n",
    "\n",
    "input_size = X_train_score.shape[2]\n",
    "d_model = 64           \n",
    "nhead = 4               \n",
    "num_encoder_layers = 3  \n",
    "dim_feedforward = 128    \n",
    "output_size = 1         \n",
    "print(input_size)\n",
    "over_under_model = TransformerTimeSeriesModel(input_size, d_model, nhead, num_encoder_layers, dim_feedforward, output_size)\n",
    "\n",
    "input_size = X_train_spread.shape[2]      \n",
    "d_model = 64           \n",
    "nhead = 4               \n",
    "num_encoder_layers = 3  \n",
    "dim_feedforward = 128    \n",
    "output_size = 1 \n",
    "print(input_size)\n",
    "spread_model = TransformerTimeSeriesModel(input_size, d_model, nhead, num_encoder_layers, dim_feedforward, output_size)\n",
    "\n",
    "def train_model(model, train_loader, imbalanced_ratio, num_epochs=300):\n",
    "    pos_weight = torch.tensor([imbalanced_ratio])\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()  # Zero gradients\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs.squeeze(), y_batch.squeeze())\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "num_positive = (y_train_score == 1).sum().item()\n",
    "num_negative = (y_train_score == 0).sum().item()\n",
    "imbalanced_ratio = num_negative / num_positive\n",
    "\n",
    "train_model(over_under_model, train_loader_score, imbalanced_ratio)\n",
    "\n",
    "num_positive = (y_train_spread == 1).sum().item()\n",
    "num_negative = (y_train_spread == 0).sum().item()\n",
    "imbalanced_ratio = num_negative / num_positive\n",
    "\n",
    "train_model(spread_model, train_loader_spread, imbalanced_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over/Under Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.60      0.61      0.60       470\n",
      "         1.0       0.51      0.51      0.51       385\n",
      "\n",
      "    accuracy                           0.56       855\n",
      "   macro avg       0.56      0.56      0.56       855\n",
      "weighted avg       0.56      0.56      0.56       855\n",
      "\n",
      "Confusion Matrix:\n",
      "[[285 185]\n",
      " [189 196]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "over_under_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = torch.round(torch.sigmoid(over_under_model(X_test_score)))\n",
    "\n",
    "# print(predictions)\n",
    "predictions = predictions.cpu().numpy().flatten()\n",
    "report = classification_report(y_test_score, predictions)\n",
    "print(\"Over/Under Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test_score, predictions)\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spread Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.62      0.62       448\n",
      "         1.0       0.58      0.58      0.58       404\n",
      "\n",
      "    accuracy                           0.60       852\n",
      "   macro avg       0.60      0.60      0.60       852\n",
      "weighted avg       0.60      0.60      0.60       852\n",
      "\n",
      "Confusion Matrix:\n",
      "[[276 172]\n",
      " [170 234]]\n"
     ]
    }
   ],
   "source": [
    "spread_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = torch.round(torch.sigmoid(spread_model(X_test_spread)))\n",
    "\n",
    "# print(predictions)\n",
    "predictions = predictions.cpu().numpy().flatten()\n",
    "report = classification_report(y_test_spread, predictions)\n",
    "print(\"Spread Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test_spread, predictions)\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
